import json
import numpy as np
import tensorflow as tf
from tensorflow.keras import layers
from kafka import KafkaConsumer

# Kafka consumer setup
consumer = KafkaConsumer(
    'mlb_parameters',
    bootstrap_servers=['localhost:9092'],
    auto_offset_reset='earliest',
    enable_auto_commit=True,
    group_id='mlb-group',
    value_deserializer=lambda x: json.loads(x.decode('utf-8'))
)

# Define the DRL model
class DRLModel(tf.keras.Model):
    def __init__(self, state_size, action_size):
        super(DRLModel, self).__init__()
        self.dense1 = layers.Dense(400, activation='relu')
        self.dense2 = layers.Dense(300, activation='relu')
        self.output_layer = layers.Dense(action_size, activation='linear')

    def call(self, state):
        x = self.dense1(state)
        x = self.dense2(x)
        return self.output_layer(x)

# Define the actor-critic framework
class ActorCritic:
    def __init__(self, state_size, action_size):
        self.actor = DRLModel(state_size, action_size)
        self.critic = DRLModel(state_size, 1)
        self.target_actor = DRLModel(state_size, action_size)
        self.target_critic = DRLModel(state_size, 1)
        self.actor_optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)
        self.critic_optimizer = tf.keras.optimizers.Adam(learning_rate=0.002)
        self.gamma = 0.99
        self.tau = 0.005

    def update_target(self):
        for target_param, param in zip(self.target_actor.trainable_variables, self.actor.trainable_variables):
            target_param.assign(self.tau * param + (1 - self.tau) * target_param)
        for target_param, param in zip(self.target_critic.trainable_variables, self.critic.trainable_variables):
            target_param.assign(self.tau * param + (1 - self.tau) * target_param)

    def train(self, state, action, reward, next_state):
        with tf.GradientTape() as tape:
            target_action = self.target_actor(next_state)
            target_q = self.target_critic(tf.concat([next_state, target_action], axis=1))
            y = reward + self.gamma * target_q
            q = self.critic(tf.concat([state, action], axis=1))
            critic_loss = tf.reduce_mean(tf.square(y - q))
        critic_grads = tape.gradient(critic_loss, self.critic.trainable_variables)
        self.critic_optimizer.apply_gradients(zip(critic_grads, self.critic.trainable_variables))

        with tf.GradientTape() as tape:
            actions = self.actor(state)
            actor_loss = -tf.reduce_mean(self.critic(tf.concat([state, actions], axis=1)))
        actor_grads = tape.gradient(actor_loss, self.actor.trainable_variables)
        self.actor_optimizer.apply_gradients(zip(actor_grads, self.actor.trainable_variables))

        self.update_target()

# Initialize model
state_size = 10  # Example state size
action_size = 5  # Example action size
agent = ActorCritic(state_size, action_size)

# Function to process Kafka messages and update model
def process_message(message):
    state = np.array(message['state'])
    action = agent.actor(state)
    reward = message['reward']
    next_state = np.array(message['next_state'])
    agent.train(state, action, reward, next_state)
    print(f"Received state: {state}, Predicted action: {action.numpy()}")

# Consume messages from Kafka
for message in consumer:
    process_message(message.value)
